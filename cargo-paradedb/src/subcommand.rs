use crate::tables::{benchlogs::EsLog, PathReader};
use anyhow::{bail, Result};
use async_std::sync::Mutex;
use async_std::task::block_on;
use cmd_lib::{run_cmd, run_fun};
use criterion::async_executor::AsyncStdExecutor;
use criterion::Criterion;
use itertools::Itertools;
use sqlx::Executor;
use sqlx::{postgres::PgConnectOptions, Connection, PgConnection, Postgres, QueryBuilder};
use std::sync::Arc;
use std::time::SystemTime;
use std::{fs, os::unix::process::CommandExt, str::FromStr};
use tempfile::tempdir;
use tracing::debug;

pub fn install() -> Result<()> {
    // The crate_path is available to us at compile time with env!.
    let crate_path = std::path::Path::new(env!("CARGO_MANIFEST_DIR")).to_path_buf();

    // We use the --offline path so that we don't update the crates.io index
    // every time we reinstall.
    let mut command = std::process::Command::new("cargo");
    command
        .arg("install")
        .arg("--offline")
        .arg("--path")
        .arg(crate_path); // Replace with the actual path

    // Using `exec` will replace the terminate the current process and replace it
    // with the command we've defined, as opposed to running it as a subprocess.
    command.exec();
    Ok(())
}

// As a note from researching the corpus generated for elasticsearch's benchmarks...
// Elasticsearch used 1024 1GB files generated by their corpus tool. However, the
// corpus tool changed its API since then, and no longer accepts a "total_bytes"
// argument. It now accepts a "total_events" argument. While more ergonomic, it means
// that we have to work backwards if we want to generate exactly 1TB of data.
pub async fn bench_eslogs_generate(
    seed: u64,
    events: u64,
    table: String,
    url: String,
) -> Result<()> {
    // Ensure that golang and the generator tool are installed.
    if let Err(err) = run_cmd!(go version > /dev/null) {
        bail!("Golang is likely not installed... {err}")
    }

    run_cmd!(go install github.com/elastic/elastic-integration-corpus-generator-tool@latest)?;

    // We're going to use the generator configuration from the elasticsearch benchmarks.
    // Download them into temporary files so they can be passed to the generator tool.
    let config_tempdir = tempdir()?;
    let template_file = config_tempdir.path().join("template.tpl");
    let fields_file = config_tempdir.path().join("fields.yml");
    let config_file = config_tempdir.path().join("config-1.yml");

    let opensearch_repo_url =
        "https://raw.githubusercontent.com/elastic/elasticsearch-opensearch-benchmark/main";

    run_cmd!(curl -s -f -o $template_file $opensearch_repo_url/dataset/template.tpl)?;
    run_cmd!(curl -s -f -o $fields_file $opensearch_repo_url/dataset/fields.yml)?;
    run_cmd!(curl -s -f -o $config_file $opensearch_repo_url/dataset/config-1.yml)?;

    // Set up necessary executable paths to call the generator tool.
    let go_path = run_fun!(go env GOPATH)?;
    let generator_exe = format!("{go_path}/bin/elastic-integration-corpus-generator-tool");

    // Set up Postgres connection and ensure the table exists.
    debug!(DATABASE_URL = url);
    let conn_opts = &PgConnectOptions::from_str(&url)?;
    let mut conn = PgConnection::connect_with(&conn_opts).await?;
    sqlx::query(&EsLog::create_table_statement(&table))
        .execute(&mut conn)
        .await?;

    // We'll skip events already created in the destination Postgres table.
    let events_already_loaded: u64 =
        sqlx::query_as::<_, (i64,)>(&format!("SELECT COUNT(id) from {}", table))
            .fetch_one(&mut conn)
            .await?
            .0 as u64;

    // The generator tool outputs to files, which we'll then read to load into Postgres.
    // We want to cap the size of the output files to a reasonable size.
    // 118891 events == 100MB of data, which is the size we'll cap each output file to.
    let events_per_file = 118891;
    let events_to_create = events - events_already_loaded;
    let files_to_create = events_to_create.div_ceil(events_per_file);
    debug!(files_to_create, events_to_create);

    // A counter for logging.
    let mut inserted_events = events_already_loaded;

    // For each generated file we intend to create, we'll run the generator tool once,
    // and then immediately load the data into Postgres.
    for file_index in 0..files_to_create {
        // Setup transaction and how many events to be generated in this transaction.
        let mut transaction = sqlx::Connection::begin(&mut conn).await?;
        let transaction_events =
            events_per_file.min(events_to_create - file_index * events_per_file);

        // We want the generated files to be deleted after inserting into Postgres,
        // so we'll make a tempdir that will be deleted when it's dropped at the
        // end of this block.
        let generated_tempdir = tempdir()?;
        let generated_dir = &generated_tempdir.path().join("generated");

        // Ensure output directory for the generated file exists.
        fs::create_dir_all(&generated_dir)?;
        // The generator tool uses the DATA_DIR env var to determine output location.
        std::env::set_var("DATA_DIR", &generated_dir);

        // The generator tool doesn't have many configuration options... including around
        // how it names files. We're stuck with the behavior that filenames will just be
        // timestamps (to the second). So if your `bytes` argument is so low that the
        // file can be generated under a second... it will just overwrite the previous file.
        // It only makes sense to `repeat` if you're generating lots of large files.
        let iter_seed = file_index + seed;
        run_cmd!(
            $generator_exe generate-with-template $template_file $fields_file
            --tot-events $transaction_events
            --config-file $config_file
            --template-type gotext
            --seed $iter_seed
            > /dev/null
        )?;

        // The files should have been generated, so build a glob string to match them.
        // The tool generates the files under a few nested folders, so make sure to
        // recursively glob for them.
        let output_files_glob_string = generated_dir.join("**/*.tpl").display().to_string();

        // Read event JSON, chunked to not overload Postgres.
        let log_chunks = EsLog::read_all(&output_files_glob_string)?.chunks(1000);

        // Build an INSERT statement and write to database.
        for chunk in log_chunks.into_iter() {
            QueryBuilder::<Postgres>::new(EsLog::insert_header(&table))
                .push_values(chunk, EsLog::insert_push_values)
                .build()
                .execute(&mut *transaction)
                .await?;
        }

        // Commit the transaction.
        transaction.commit().await?;

        // Log inserted events.
        inserted_events += transaction_events;
        debug!(inserted = inserted_events, "inserting json benchlog chunk");
    }
    Ok(())
}

pub async fn bench_eslogs_build_search_index(
    table: String,
    index: String,
    url: String,
) -> Result<()> {
    let drop_query = format!("CALL paradedb.drop_bm25('{index}')");

    let text_fields = r#"{"message": {}}"#;
    let create_query = format!(
        "CALL paradedb.create_bm25(
            table_name => '{table}',
            index_name => '{index}',
            key_field => 'id',
            text_fields => '{text_fields}'
        );"
    );

    Benchmark {
        group_name: "Search Index".into(),
        function_name: "bench_eslogs_build_search_index".into(),
        // First, drop any existing index to ensure a clean environment.
        setup_query: Some(drop_query),
        query: create_query,
        database_url: url.into(),
    }
    .run_once()
    .await
}

pub async fn bench_eslogs_query_search_index(
    index: String,
    query: String,
    limit: u64,
    url: String,
) -> Result<()> {
    Benchmark {
        group_name: "Search Query".into(),
        function_name: "bench_eslogs_query_search_index".into(),
        setup_query: None,
        query: format!("SELECT * FROM {index}.search('{query}', limit_rows => {limit});"),
        database_url: url,
    }
    .run()
    .await
}

pub async fn bench_eslogs_build_parquet_table(table: String, url: String) -> Result<()> {
    let parquet_table_name = format!("{table}_parquet");
    let drop_query = format!(
        r#"
        DROP TABLE IF EXISTS {parquet_table_name};
        CREATE TABLE {parquet_table_name} (metrics_size int4) USING parquet;
        "#
    );
    let create_query = format!(
        r#"
        INSERT INTO {parquet_table_name} (metrics_size)
        SELECT metrics_size FROM {table};
        "#
    );

    Benchmark {
        group_name: "Parquet Table".into(),
        function_name: "bench_eslogs_build_parquet_table".into(),
        // First, drop any existing table to ensure a clean environment.
        setup_query: Some(drop_query),
        query: create_query,
        database_url: url.into(),
    }
    .run_once()
    .await
}

pub async fn bench_eslogs_count_parquet_table(table: String, url: String) -> Result<()> {
    Benchmark {
        group_name: "Parquet Table".into(),
        function_name: "bench_eslogs_build_parquet_table".into(),
        setup_query: None, // First, drop any existing index to ensure a clean environment.
        query: format!("SELECT COUNT(*) FROM {table}_parquet"),
        database_url: url.into(),
    }
    .run()
    .await
}

struct Benchmark {
    group_name: String,
    function_name: String,
    setup_query: Option<String>,
    query: String,
    database_url: String,
}

impl Benchmark {
    pub async fn setup_query(&self, conn: &mut PgConnection) -> Result<()> {
        if let Some(query) = &self.setup_query {
            conn.execute(query.as_ref()).await?;
        }

        Ok(())
    }
    pub async fn run(&self) -> Result<()> {
        // One-time setup code goes here.
        debug!(DATABASE_URL = self.database_url);
        let mut criterion = Criterion::default();
        let mut group = criterion.benchmark_group(&self.group_name);

        // Lowered from default sample size to remove Criterion warning.
        // Must be higher than 10, or Criterion will panic.
        group.sample_size(60);
        group.bench_function(&self.function_name, |runner| {
            // Per-sample (note that a sample can be many iterations) setup goes here.
            let conn_opts = &PgConnectOptions::from_str(&self.database_url).unwrap();
            let conn = block_on(async {
                Arc::new(Mutex::new(
                    PgConnection::connect_with(&conn_opts).await.unwrap(),
                ))
            });

            // Run setup query.
            block_on(async {
                let local_conn = conn.clone();
                let mut conn = local_conn.lock().await; // Acquire the lock asynchronously.
                self.setup_query(&mut conn).await.unwrap();
            });

            runner.to_async(AsyncStdExecutor).iter(|| {
                // Measured code goes here.
                async {
                    let local_conn = conn.clone();
                    let mut conn = local_conn.lock().await; // Acquire the lock asynchronously.
                    sqlx::query(&self.query).execute(&mut *conn).await.unwrap();
                }
            });
        });

        group.finish();

        Ok(())
    }

    async fn run_once(&self) -> Result<()> {
        let conn_opts = &PgConnectOptions::from_str(&self.database_url).unwrap();
        let mut conn = PgConnection::connect_with(&conn_opts).await.unwrap();

        // Run setup query if present.
        self.setup_query(conn.as_mut()).await.unwrap();

        // Run actual query to be benchmarked.
        let start_time = SystemTime::now();
        block_on(async {
            sqlx::query(&self.query).execute(&mut conn).await.unwrap();
        });
        let end_time = SystemTime::now();

        if let Ok(duration) = end_time.duration_since(start_time) {
            println!("Start time: {:?}", start_time);
            println!("End time: {:?}", end_time);

            let milliseconds = duration.as_millis();
            let seconds = duration.as_secs_f64(); // Use floating point for seconds
            let minutes = seconds / 60.0; // Convert seconds to minutes
            let hours = seconds / 3600.0; // Convert seconds to hours

            println!("Duration: {} milliseconds", milliseconds);
            println!("Duration: {:.4} seconds", seconds); // Print with 4 decimal places
            println!("Duration: {:.4} minutes", minutes); // Print with 4 decimal places
            println!("Duration: {:.4} hours", hours); // Print with 4 decimal places
        } else {
            println!("An error occurred while calculating the duration.");
        }

        Ok(())
    }
}
