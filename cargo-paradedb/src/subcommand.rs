use crate::tables::{benchlogs::EsLog, PathReader};
use anyhow::{bail, Result};
use cmd_lib::{run_cmd, run_fun};
use itertools::Itertools;
use sqlx::{postgres::PgConnectOptions, Connection, PgConnection, Postgres, QueryBuilder};
use std::{fs, os::unix::process::CommandExt, str::FromStr, time::SystemTime};
use tempfile::tempdir;
use tracing::debug;

pub fn install() -> Result<()> {
    // The crate_path is available to us at compile time with env!.
    let crate_path = std::path::Path::new(env!("CARGO_MANIFEST_DIR")).to_path_buf();

    // We use the --offline path so that we don't update the crates.io index
    // every time we reinstall.
    let mut command = std::process::Command::new("cargo");
    command
        .arg("install")
        .arg("--offline")
        .arg("--path")
        .arg(crate_path); // Replace with the actual path

    // Using `exec` will replace the terminate the current process and replace it
    // with the command we've defined, as opposed to running it as a subprocess.
    command.exec();
    Ok(())
}

// As a note from researching the corpus generated for elasticsearch's benchmarks...
// Elasticsearch used 1024 1GB files generated by their corpus tool. However, the
// corpus tool changed its API since then, and no longer accepts a "total_bytes"
// argument. It now accepts a "total_events" argument. While more ergonomic, it means
// that we have to work backwards if we want to generate exactly 1TB of data.
pub async fn bench_eslogs_generate(
    seed: u64,
    events: u64,
    table: String,
    url: String,
) -> Result<()> {
    // Ensure that golang and the generator tool are installed.
    if let Err(err) = run_cmd!(go version > /dev/null) {
        bail!("Golang is likely not installed... {err}")
    }

    run_cmd!(go install github.com/elastic/elastic-integration-corpus-generator-tool@latest)?;

    // We're going to use the generator configuration from the elasticsearch benchmarks.
    // Download them into temporary files so they can be passed to the generator tool.
    let config_tempdir = tempdir()?;
    let template_file = config_tempdir.path().join("template.tpl");
    let fields_file = config_tempdir.path().join("fields.yml");
    let config_file = config_tempdir.path().join("config-1.yml");

    let opensearch_repo_url =
        "https://raw.githubusercontent.com/elastic/elasticsearch-opensearch-benchmark/main";

    run_cmd!(curl -s -f -o $template_file $opensearch_repo_url/dataset/template.tpl)?;
    run_cmd!(curl -s -f -o $fields_file $opensearch_repo_url/dataset/fields.yml)?;
    run_cmd!(curl -s -f -o $config_file $opensearch_repo_url/dataset/config-1.yml)?;

    // Set up necessary executable paths to call the generator tool.
    let go_path = run_fun!(go env GOPATH)?;
    let generator_exe = format!("{go_path}/bin/elastic-integration-corpus-generator-tool");

    // Set up Postgres connection and ensure the table exists.
    debug!(DATABASE_URL = url);
    let conn_opts = &PgConnectOptions::from_str(&url)?;
    let mut conn = PgConnection::connect_with(&conn_opts).await?;
    sqlx::query(&EsLog::create_table_statement(&table))
        .execute(&mut conn)
        .await?;

    // We'll skip events already created in the destination Postgres table.
    let events_already_loaded: u64 =
        sqlx::query_as::<_, (i64,)>(&format!("SELECT COUNT(id) from {}", table))
            .fetch_one(&mut conn)
            .await?
            .0 as u64;

    // The generator tool outputs to files, which we'll then read to load into Postgres.
    // We want to cap the size of the output files to a reasonable size.
    // 118891 events == 100MB of data, which is the size we'll cap each output file to.
    let events_per_file = 118891;
    let events_to_create = events - events_already_loaded;
    let files_to_create = events_to_create.div_ceil(events_per_file);
    debug!(files_to_create, events_to_create);

    // A counter for logging.
    let mut inserted_events = events_already_loaded;

    // For each generated file we intend to create, we'll run the generator tool once,
    // and then immediately load the data into Postgres.
    for file_index in 0..files_to_create {
        // Setup transaction and how many events to be generated in this transaction.
        let mut transaction = sqlx::Connection::begin(&mut conn).await?;
        let transaction_events =
            events_per_file.min(events_to_create - file_index * events_per_file);

        // We want the generated files to be deleted after inserting into Postgres,
        // so we'll make a tempdir that will be deleted when it's dropped at the
        // end of this block.
        let generated_tempdir = tempdir()?;
        let generated_dir = &generated_tempdir.path().join("generated");

        // Ensure output directory for the generated file exists.
        fs::create_dir_all(&generated_dir)?;
        // The generator tool uses the DATA_DIR env var to determine output location.
        std::env::set_var("DATA_DIR", &generated_dir);

        // The generator tool doesn't have many configuration options... including around
        // how it names files. We're stuck with the behavior that filenames will just be
        // timestamps (to the second). So if your `bytes` argument is so low that the
        // file can be generated under a second... it will just overwrite the previous file.
        // It only makes sense to `repeat` if you're generating lots of large files.
        let iter_seed = file_index + seed;
        run_cmd!(
            $generator_exe generate-with-template $template_file $fields_file
            --tot-events $transaction_events
            --config-file $config_file
            --template-type gotext
            --seed $iter_seed
            > /dev/null
        )?;

        // The files should have been generated, so build a glob string to match them.
        // The tool generates the files under a few nested folders, so make sure to
        // recursively glob for them.
        let output_files_glob_string = generated_dir.join("**/*.tpl").display().to_string();

        // Read event JSON, chunked to not overload Postgres.
        let log_chunks = EsLog::read_all(&output_files_glob_string)?.chunks(1000);

        // Build an INSERT statement and write to database.
        for chunk in log_chunks.into_iter() {
            QueryBuilder::<Postgres>::new(EsLog::insert_header(&table))
                .push_values(chunk, EsLog::insert_push_values)
                .build()
                .execute(&mut *transaction)
                .await?;
        }

        // Commit the transaction.
        transaction.commit().await?;

        // Log inserted events.
        inserted_events += transaction_events;
        debug!(inserted = inserted_events, "inserting json benchlog chunk");
    }
    Ok(())
}

pub async fn bench_eslogs_build_search_index(
    table_name: String,
    index_name: String,
    url: String,
) -> Result<()> {
    // Set up Postgres connection and ensure the table exists.
    debug!(DATABASE_URL = url);
    let conn_opts = &PgConnectOptions::from_str(&url)?;
    let mut conn = PgConnection::connect_with(&conn_opts).await?;

    // First, drop any existing index to ensure a clean environment.
    let drop_query = format!("CALL paradedb.drop_bm25('{index_name}')");
    sqlx::query(&drop_query).execute(&mut conn).await?;

    // Start a timer for the benchmark.
    let start_time = SystemTime::now();

    let text_fields = r#"{"message": {}}"#;
    let create_query = format!(
        "CALL paradedb.create_bm25(
            table_name => '{table_name}',
            index_name => '{index_name}',
            key_field => 'id',
            text_fields => '{text_fields}'
        );"
    );

    sqlx::query(&create_query).execute(&mut conn).await?;

    // Benchmark ends here
    let end_time = SystemTime::now();
    print_results(start_time, end_time);

    Ok(())
}

fn print_results(start_time: SystemTime, end_time: SystemTime) {
    if let Ok(duration) = end_time.duration_since(start_time) {
        println!("Start time: {:?}", start_time);
        println!("End time: {:?}", end_time);

        let milliseconds = duration.as_millis();
        let seconds = duration.as_secs_f64(); // Use floating point for seconds
        let minutes = seconds / 60.0; // Convert seconds to minutes
        let hours = seconds / 3600.0; // Convert seconds to hours

        println!("Duration: {} milliseconds", milliseconds);
        println!("Duration: {:.4} seconds", seconds); // Print with 4 decimal places
        println!("Duration: {:.4} minutes", minutes); // Print with 4 decimal places
        println!("Duration: {:.4} hours", hours); // Print with 4 decimal places
    } else {
        println!("An error occurred while calculating the duration.");
    }
}
